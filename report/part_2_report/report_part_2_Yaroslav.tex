% encoding: utf-8
\documentclass[12pt,a4paper]{article}
% поддержка русского
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{cmap}            % копирование кириллицы из PDF
% математические пакеты
\usepackage{amsmath,amssymb}
\usepackage[fleqn]{amsmath}  % выравнивание формул слева
% графика
\usepackage{graphicx}
\graphicspath{{./}}
% таблицы
\usepackage{booktabs}
\usepackage{array}
% гиперссылки
\usepackage[hidelinks]{hyperref}
% отступ первой строки абзаца
\setlength{\parindent}{1em}
% расстояние между абзацами
\setlength{\parskip}{0.5em}
% заголовок
\title{Отчет по части II}
\author{Богданов Ярослав}
\date{} % убрать дату

\begin{document}
\maketitle


\textbf{Гипотезы:}
\begin{itemize}
    \item $H_0$: данные из распределения weibull с параметром $\lambda = 1$
    \item $H_1$: данные из распределения lognormal с параметром $\sigma =log(5)$
\end{itemize}

\textbf{Параметры исследования:}
\begin{itemize}
    \item Тип графа: dist-граф с параметром $d = 0.5$
    \item Размеры выборок: $n = 25, 100, 500$
    \item Количество выборок на класс: 100
\end{itemize}

\textbf{Исследуемые характеристики графов:}
\begin{itemize}
    \item $\Delta(G)$ --- максимальная степень вершины
    \item $\delta(G)$ --- минимальная степень вершины  
    \item $c(G)$ --- количество компонент связности
    \item $t(G)$ --- количество треугольников
    \item $\text{diam}(G)$ --- диаметр графа
    \item $\lambda(G)$ --- рёберная связность
    \item $\omega(G)$ --- кликовое число
\end{itemize}

\section{Результаты}

\subsection{Анализ важности характеристик}

Анализ важности характеристик с использованием Random Forest показал следующие результаты:\\

\includegraphics[width=1\linewidth]{feature_importance.jpg}
    

\textbf{Основные наблюдения:}
\begin{itemize}
    \item При $n=25$ наибольший вклад дают число треугольников $t(G)\approx29\%$ и кликовое число $\omega(G)\approx27\%$, за ними следуют максимальная степень $\Delta(G)\approx17\%$ и диаметр $\mathrm{diam}(G)\approx13\%$.
    \item При $n=100$ доминирует $\omega(G)\approx46\%$, тогда как важность диаметра $\mathrm{diam}(G)$ падает до $\approx4\%$, а $\delta(G)$ и $\lambda(G)$ практически сходят на нет ($<1\%$).
    \item При увеличении до $n=500$ максимальная степень $\Delta(G)$ растёт до $\approx32\%$ и выравнивается с $\omega(G)\approx32\%$, число треугольников $t(G)$ остаётся стабильным ($\approx29\%$).
    \item Диаметр $\mathrm{diam}(G)$ продолжает снижаться (до $\approx1.3\%$), а минимальная степень $\delta(G)$ и реберная связность $\lambda(G)$ практически теряют значение.
    \item Число компонент связности $c(G)$ демонстрирует U-образную динамику: $\approx7\%\to2\%\to6.5\%$ при росте $n$.
\end{itemize}


   
\subsection{Сравнение классификаторов}

Для оценки качества классификации использовались следующие алгоритмы: Random Forest, Logistic Regression и Neural Network. Результаты представлены на графике:

\includegraphics[width=1\linewidth]{comparison.jpg}

\textbf{Основные выводы по классификаторам:}
\begin{itemize}
    \item Для малых выборок ($n=25$) все модели демонстрируют среднюю точность ($\approx0.75$–$0.77$) и аналогичный F1-Score, при этом ошибка I рода значительно превышает уровень значимости ($\alpha=0.05$), достигая $20\%$–$24\%$, а мощность критерия находится на уровне $0.72$–$0.74$.
    \item При увеличении выборки до $n=100$ точность и F1-Score резко возрастают до $0.95$–$0.97$, ошибка I рода падает ниже $5\%$ (до $\approx2\%$–$3\%$), а мощность критерия достигает $0.94$–$0.97$.
    \item Для больших выборок ($n=500$) все три алгоритма достигают практически идеальных показателей: точность и F1-Score близки к $1.00$, ошибка I рода стремится к нулю, мощность критерия приближается к единице.
    \item Различия между алгоритмами минимальны: Logistic Regression чуть опережает Random Forest на средних выборках, Neural Network демонстрирует чуть больший разброс оценок.
\end{itemize}

\subsection{Анализ распределений характеристик}
\includegraphics[width=0.7\linewidth]{dists.png}\\\\\\
Гистограммы распределений характеристик графов показывают, как изменяется разделимость между гипотезами $H_0$ и $H_1$ с ростом размера выборки:
\begin{itemize}
    \item $\Delta(G)$ (максимальная степень) — при $n=25$ видна лишь слабая тенденция к сдвигу, при $n=100$ распределения уже хорошо разделяются, а при $n=500$ их разделение становится почти полным.
    \item $t(G)$ (число треугольников) — умеренное разделение для $n=25$ и $n=100$, для $n=500$ гистограммы практически не перекрываются.
    \item $\mathrm{diam}(G)$ (диаметр) — заметное, но неполное разделение; с ростом $n$ средние значения расходятся, но хвосты всё ещё пересекаются.
    \item $\omega(G)$ (кликовое число) — при $n=100$ уже явное разделение, при $n=500$ гистограммы хорошо разделены.
    \item $c(G)$ (число компонент связности) — небольшое смещение средних при $n\ge100$, сильнее выраженное при $n=500$, но перекрытие сохраняется.
    \item $\delta(G)$ (минимальная степень) и $\lambda(G)$ (рёберная связность) — при любых $n$ распределения почти совпадают, разделения не наблюдается.
\end{itemize}
С увеличением размера выборки разделение между распределениями становится более выраженным, что объясняет улучшение качества классификации на больших $n$.

\section{Выводы}

Анализ итоговых показателей классификации даёт следующие выводы:

\begin{itemize}
    \item \textbf{Для $n=25$}: ни один классификатор не удовлетворяет условию $\alpha \leq 0.05$.
    \item \textbf{Для $n=100$}: лучший классификатор — Random Forest с ошибкой I рода $\alpha = 0.0344$ и мощностью $0.9722$.
    \item \textbf{Для $n=500$}: лучший классификатор — Random Forest с ошибкой I рода $\alpha = 0.0011$ и мощностью $1.0000$.
\end{itemize}


\end{document}
